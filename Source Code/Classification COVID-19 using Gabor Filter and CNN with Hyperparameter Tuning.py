# -*- coding: utf-8 -*-
"""Diagnosis COVID 19 using Gabor Filter and CNN with Hyperparameter Tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OQzcitzofstD3V4qMs5tr6mMfqPJ9YTk

**PORTOFOLIO | Diagnosis COVID 19 using Gabor Filter and CNN with Hyperparameter Tuning** \
**Tema** : Computer Vision | Data Science \
**Author** : Mochammad Hazmi Cokro Mandiri \
**Judul** : **Diagnosis COVID 19 using Gabor Filter and CNN with Hyperparameter Tuning** 


---


**Dataset** : [Sars COV 2 - CT SCAN Dataset](https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset) \
**Format Data** : Image ".png" \
**Total Data** : 2481  \
**Categories** : 2 Class = COVID (1) , Non-COVID (0))

#**[*] Library**
**Melakukan Import modul-modul yang dibutuhkan untuk pengerjaan**
"""

# *
import os
import cv2
import shutil
import random
import numpy as np
import pandas as pd
from pandas import DataFrame
from google.colab import files
from google.colab import drive

# u/ Preprocessing data
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PIL import Image
from keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.utils import np_utils
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.image import load_img

# u/ Hyperparameter Tuning
import tensorflow as tf
from tensorboard.plugins.hparams import api as hp

# u/ Melatih / Training data (Model)
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation,GlobalMaxPool2D
from tensorflow.keras.layers import Dense, Flatten, BatchNormalization
from tensorflow.keras.layers import Dropout, Input
from tensorflow.keras.optimizers import Adam, RMSprop, Adamax, SGD
from keras.models import load_model
from tensorflow.keras import callbacks

# u/ Evaluasi Model
import itertools
from sklearn import metrics
from sklearn.metrics import f1_score, recall_score, accuracy_score
from sklearn.metrics import precision_score, confusion_matrix, classification_report
from sklearn.metrics import  precision_recall_curve, auc, roc_curve, roc_auc_score

"""#**[1] Preparation**
**Menyiapkan dataset (download dan split dataset)**

###**[1.1] Mount Google Drive**
**Menghubungkan ke google drive dan mengatur area lokasi kerja (workspace)**
"""

drive.mount('/content/drive')

os.environ['KAGGLE_CONFIG_DIR'] = "/content/drive/MyDrive/[Trial] Simponia"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/[Trial] Simponia

"""###**[1.2] Download dan Ekstrak Dataset**
**Mengunduh dan mengekstrak dataset dari Kaggle**
"""

from google.colab import files
files.upload()
print('Sukses Upload Kaggle.JSON')

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!ls ~/.kaggle

!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d plameneduardo/sarscov2-ctscan-dataset

!ls

!unzip -q sarscov2-ctscan-dataset.zip

!ls

!mkdir DataCovid
print('Sukses buat folder DataCovid')
!mv COVID non-COVID -t DataCovid
print('\nSukses memindahkan Covid - Non Covid ke folder DataCovid')

!mkdir Dataset
print('Sukses buat folder Dataset Covid')

"""###**[1.3] Split Dataset**
**Melakukan pembagian data menjadi ratio 80% Train, 10% Test dan 10% Validation**
"""

!pip install split-folders

import splitfolders
splitfolders.ratio('/content/drive/MyDrive/[Trial] Simponia/DataCovid', output='/content/drive/MyDrive/[Trial] Simponia/Dataset',
                   seed=121, ratio=(.8, .1, .1), group_prefix=None)

"""Mengecek Ukuran Gambar"""

from PIL import Image

im = Image.open('/content/drive/MyDrive/[Trial] Simponia/Dataset/train/COVID/Covid (1001).png')
im2 = Image.open('/content/drive/MyDrive/[Trial] Simponia/Dataset/test/non-COVID/Non-Covid (1016).png')
print(im.size)
print(im2.size)

"""#**[2] Preprocessing**
**Melakukan Gathering Data, Resize Image**

###**[2.1] Plot Random Image tiap Kelas**
**Menampilkan gambar random tiap kelas**
"""

covid_dir = os.path.join("/content/drive/MyDrive/[Trial] Simponia/Dataset/val/COVID/")  
noncovid_dir = os.path.join("/content/drive/MyDrive/[Trial] Simponia/Dataset/val/non-COVID/")

list1 = [covid_dir,noncovid_dir]

fig = plt.figure(figsize=(12, 8))

j=1
for i in list1: 
    for k in range(4):
        filenames  = os.listdir(i)
        sample = random.choice(filenames)
        image = load_img(i+sample)
        plt.subplot(2,4,j)
        plt.imshow(image)
        plt.xlabel(i.split("/")[-2])
        j+=1
plt.tight_layout()

"""**Pada Plot Random Gambar diatas terlihat bahwa gambar memiliki ukuran yang berbeda beda maka akan dilakukan resizing gambar menjadi ukuran 128 x 128 untuk meyeragamkan ukuran gambar**

###**[2.2] Gathering Data , Resizing image**
**Melakukan gathering data dengan membentuk data train, data test dan data validation yang berisi gambar dan label serta melakukan resizing image menjadi ukuran 128x128 untuk menyeragamkan ukuran gambar.**

Mengatur lokasi / path data
"""

training_dir = r"/content/drive/MyDrive/[Trial] Simponia/Dataset/train/"
validation_dir = r"/content/drive/MyDrive/[Trial] Simponia/Dataset/val/"
testing_dir = r"/content/drive/MyDrive/[Trial] Simponia/Dataset/test/"

categories = ['non-COVID', 'COVID']

"""Melakukan Gathering dan Resize menjadi 128x128"""

# Mendeklarasikan ukuran gambar
img_size = (128,128)

training_data = []
validation_data = []
testing_data = []

def create_training_data():
    for category in categories:
        path = os.path.join(training_dir,category)
        class_num = categories.index(category)
        for img in os.listdir(path):
            try:
                img_array = cv2.imread(os.path.join(path,img))
                new_array = cv2.resize(img_array,img_size) 
                training_data.append([new_array,class_num])
            except Exception as e:
                pass

def create_validation_data():
    for category in categories:
        path = os.path.join(validation_dir,category)
        class_num = categories.index(category)
        for img in os.listdir(path):
            try:
                img_array = cv2.imread(os.path.join(path,img))
                new_array = cv2.resize(img_array,img_size) 
                validation_data.append([new_array,class_num])
            except Exception as e:
                pass

def create_testing_data():
    for category in categories:
        path = os.path.join(testing_dir,category)
        class_num = categories.index(category)
        for img in os.listdir(path):
            try:
                img_array = cv2.imread(os.path.join(path,img))
                new_array = cv2.resize(img_array,img_size) 
                testing_data.append([new_array,class_num])
            except Exception as e:
                pass


create_training_data()
print("Berhasil Create Training Data")
create_validation_data()
print("Berhasil Create Validation Data")
create_testing_data()
print("Berhasil Create Validation Data")

"""###**[2.3] Normalisasi dan Labeling Data**
**Melakukan Normalisasi data menjadi range 0-1 dan labeling data**

Melakukan Normalisasi data dimana setiap pixel yang memiliki range 0-255 akan dinormalisasikan menjadi range 0-1
"""

# Melakukan pengambilan features/gambar dan label dari masing" kelas

X_train = []
Y_train = []
for features,label in training_data:
    X_train.append(features)
    Y_train.append(label)
X_train = np.array(X_train).reshape(-1,128,128)
X_train = X_train.astype('float32')/255.0  
X_train = X_train.reshape(-1,128,128,3)
print(X_train.shape)

X_val = []
Y_val = []
for features,label in validation_data:
    X_val.append(features)
    Y_val.append(label)
X_val = np.array(X_val).reshape(-1,128,128)
X_val = X_val.astype('float32')/255.0  
X_val = X_val.reshape(-1,128,128,3)
print(X_val.shape)

X_test = []
Y_test = []
for features,label in testing_data:
    X_test.append(features)
    Y_test.append(label)
X_test = np.array(X_test).reshape(-1,128,128)
X_test = X_test.astype('float32')/255.0  
X_test = X_test.reshape(-1,128,128,3)
print(X_test.shape)

"""Labeling Data menggunakan label encoder untuk Y_train/val/test"""

lb = LabelEncoder()

Y_train = lb.fit_transform(Y_train)
Y_val = lb.fit_transform(Y_val)
Y_test = lb.fit_transform(Y_test)

"""#**[3] Augmentasi Data**
**Implementasi Augmentasi Data menggunakan library Keras**

Augmentasi data menggunakan library dari Keras bernama ImageDataGenerator
"""

datagen = ImageDataGenerator(
        rotation_range = 30,
        zoom_range = 0.1,
        width_shift_range=0.1,
        height_shift_range=0.1,
        horizontal_flip=True,  
        vertical_flip=False)

"""#**[4] Filter Gabor**
**Melakukan Implementasi Filter Gabor**

Membentuk fungsi grayscale untuk mengubah gambar menjadi warna grayscale
"""

def grayscale(data, dtype='float32'):
    r, g, b = np.asarray(.3, dtype=dtype), np.asarray(.59, dtype=dtype), np.asarray(.11, dtype=dtype)
    rst = r * data[:, :, :, 0] + g * data[:, :, :, 1] + b * data[:, :, :, 2]
    return rst

"""Membentuk fungsi add_dimension untuk mengubah urutan matrix"""

def add_dimension(data):
    data = np.array([data])
    print(data.shape)
    data = np.einsum('hijk->ijkh', data)
    return data

"""Implementasi filter gabor dengan membuat fungsi bernama custom gabor yang menerapkan Garborkernel dari library Open CV"""

def custom_gabor(shape, dtype=None):
    pi = np.pi
    orientation_spread = np.array([0, pi/4, pi/2, pi*3/4, pi, pi*5/4, pi*3/2, 2*pi])
    scales = np.linspace(2, 4, 2)
    real_kernels = []
    img_kernels = []
    for orientation in orientation_spread:
        for scale in scales:
            real_kernel = cv2.getGaborKernel((3, 3), 1, orientation, scale, 1, 0)
            imaginary_kernel = cv2.getGaborKernel((3, 3), 1, orientation, scale, 1, np.pi / 2)
            real_kernels.append(real_kernel)
            img_kernels.append(imaginary_kernel)
    stacked_list = np.vstack((real_kernels, img_kernels))
    stacked_list = np.array([stacked_list])
    stacked_list = np.einsum('hijk->jkhi', stacked_list)
    print(stacked_list.shape)

    stacked_list = K.variable(stacked_list)
    random = K.random_normal(shape, dtype=dtype)
    return stacked_list

"""Melakukan penerapan filter gabor pada data train test dan validation sehingga data akan berubah menjadi 1 channel warna"""

X_train_gabor = add_dimension(grayscale(X_train))
X_val_gabor = add_dimension(grayscale(X_val))
X_test_gabor = add_dimension(grayscale(X_test))
Y_train_gabor = Y_train
Y_val_gabor = Y_val
Y_test_gabor = Y_test

"""#**[5] HyperParameter Tuning**
**Melakukan hyperparameter Tuning untuk mendapatkan parameter terbaik untuk model CNN yang diusung**

"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

!rm -rf ./logs/

"""###**[5.1] Parameter Pembanding**
**Mendeklarasikan parameter-parameter pembanding yang akan diuji coba**

\

| Parameter Pembanding | Value |
| ------------- | ------------- |
| Optimizer	| Adam, SGD, Adamax | 
| Dropout setelah pooling layer (APL Dropout) |	0.05, 0,1 |
| Dropout fully connected layer	(Dropout) | 0.25, 0,1 |
|Dense layer |	64, 128 |

"""

HP_APL_DROPOUT = hp.HParam('dropout_apl', hp.RealInterval(0.05, 0.1))
HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([64, 128]))
HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.25, 0.5))
HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'adamax']))

METRIC_ACCURACY = 'accuracy'

with tf.summary.create_file_writer('logs/hparam_tuning').as_default():
  hp.hparams_config(
    hparams=[HP_APL_DROPOUT, HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],
    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],
  )

"""###**[5.2] Model Usulan**
**Melakukan deklarasi model yang diusulkan untuk dilakukan pencarian best parameter dari parameter" pembanding sebelumnya.**
"""

def train_test_model(hparams):

  model = Sequential([
      Input(shape=(128,128,3)),

      Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),

      Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='swish'),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(hparams[HP_APL_DROPOUT]),

      Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),

      Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(hparams[HP_APL_DROPOUT]),

      Conv2D(filters=256, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(0.05),
       
      Conv2D(filters=256, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(hparams[HP_APL_DROPOUT]),
       
      GlobalMaxPool2D(),
      Flatten(),

      Dense(hparams[HP_NUM_UNITS], activation="swish"),
      Dense(128, activation = 'swish'),
      Dropout(hparams[HP_DROPOUT]),
      Dense(1, activation='sigmoid')
      ]
  )
  
  model.compile(
      optimizer=hparams[HP_OPTIMIZER],
      loss='binary_crossentropy',
      metrics=['accuracy'],
  )

  datagen.fit(X_train)
  history = model.fit_generator(
                              datagen.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = 5, 
                              validation_data = datagen.flow(X_val,Y_val))
  _, accuracy = model.evaluate(X_val, Y_val)
  return accuracy

"""###**[5.3] Searching Best Parameter**
**Melakukan pencarian parameter terbaik untuk model dengan cara melatih model tersebut dengan parameter" pembanding**
"""

def run(run_dir, hparams):
  with tf.summary.create_file_writer(run_dir).as_default():
    hp.hparams(hparams)
    accuracy = train_test_model(hparams)
    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=50)

session_num = 0

for dropout_apl_rate in (HP_APL_DROPOUT.domain.min_value, HP_APL_DROPOUT.domain.max_value):
  for num_units in HP_NUM_UNITS.domain.values:
    for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):
      for optimizer in HP_OPTIMIZER.domain.values:
        hparams = {
            HP_APL_DROPOUT: dropout_apl_rate,
            HP_NUM_UNITS: num_units,
            HP_DROPOUT: dropout_rate,
            HP_OPTIMIZER: optimizer,
        }
        run_name = "run-%d" % session_num
        print('--- Starting trial: %s' % run_name)
        print({h.name: hparams[h] for h in hparams})
        run('logs/hparam_tuning/' + run_name, hparams)
        session_num += 1

"""###**[5.4] Visualisasi Hasil Tuning**
**Melakukan visualisasi hasil Hyperparameter Tuning menggunakan tensorboard untuk memudahkan dalam menemukan parameter-parameter terbaik**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# wget -q 'https://storage.googleapis.com/download.tensorflow.org/tensorboard/hparams_demo_logs.zip'
# unzip -q hparams_demo_logs.zip -d logs/hparam_demo

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/hparam_tuning

"""#[6] Processing

##**[6.1] Skenario Model 1**
**Model Usulan** \
Optimizer : SGD , APL Dropout : 0.05, Dropout : 0.5, Dense : 128

Skenario Model 1 - Sequential
"""

model1 = Sequential(
      [
      Input(shape=(128,128,3)),

      Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),

      Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='swish'),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(0.05),

      Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),

      Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(0.05),

      Conv2D(filters=256, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(0.05),
       
      Conv2D(filters=256, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(0.05),
       
      GlobalMaxPool2D(),
      Flatten(),

      Dense(128, activation = 'swish'),
      Dense(128, activation = 'swish'),
      Dropout(0.5),
      Dense(1, activation='sigmoid')
      ]
  )

"""Skenario model 1 menggunakan Optimizer SGD dengan learning rate 0.00146 dan loss binary crossentropy"""

SGD(learning_rate=0.00146, name='SGD')
model1.compile(optimizer = 'SGD',loss = 'binary_crossentropy',metrics = ['accuracy'])

"""Implementasi Callback berupa Learning rate Scheduler"""

def scheduler(epoch, lr):
  if epoch < 5:
    return lr
  else:
    return lr * tf.math.exp(-0.1)

lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)
tb_callback = tf.keras.callbacks.TensorBoard(
    log_dir='logs', histogram_freq=0, write_graph=True, write_images=False,
    update_freq='epoch', embeddings_freq=0,
    embeddings_metadata=None
)

"""###**[6.1.1] Training Skenario 1**"""

import time
start = time.time()

batch_size = 32
epochs = 100  

datagen.fit(X_train)
history1 = model1.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (X_val,Y_val),
                              steps_per_epoch = X_train.shape[0] // batch_size,
                              callbacks=[lr_schedule, tb_callback])
stop = time.time()

"""Saving model"""

model1.save('Model_Skenario1.h5')

# model3 = load_model('model_3_2.h5')

"""###**[6.1.2] Visualisasi Skenario 1**"""

acc = history1.history["accuracy"]
val_acc = history1.history["val_accuracy"]
loss = history1.history["loss"]
val_loss = history1.history["val_loss"]

epochs = range(len(acc))

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 5])
ax1.plot(epochs, acc, 'k')
ax1.plot(epochs, val_acc, 'c')
ax1.set_title('Accuracy | Skenario 1')
ax1.legend(['Model accuracy','Model Val accuracy'])

ax2.plot(epochs, loss, 'k')
ax2.plot(epochs, val_loss, 'c')
ax2.set_title('Loss | Skenario 1')
ax2.legend(['Model loss','Model Val loss'])
plt.savefig('skenario1.png')

plt.show()

"""###**[6.1.3] Evaluasi Skenario 1**
**Melakukan evaluasi model menggunakan evaluate, classification report, confusion matrix dan roc curve**

Evaluasi pada Data Train
"""

loss, acc = model1.evaluate(X_train,Y_train,verbose = 0)
print("Training Loss {:.5f} dan Training Accuracy {:.2f}%".format(loss,acc*100))

"""Evaluasi pada Data Validation"""

loss, acc = model1.evaluate(X_val,Y_val,verbose = 0)
print("Validation Loss {:.5f} dan Validation Accuracy {:.2f}%".format(loss,acc*100))

"""Evaluasi pada Data Test"""

loss, acc = model1.evaluate(X_test,Y_test,verbose = 0)
print("Test Loss {:.5f} dan Test Accuracy {:.2f}%".format(loss,acc*100))

"""Classification Report"""

print("Skenario Model 1")
pred = model1.predict(X_val)
labels = (pred > 0.5).astype(np.int)

print(classification_report(Y_val, labels, target_names = categories))

"""Confusion Matrix"""

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.BuGn):

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

confusion_mtx = confusion_matrix(Y_val, labels)
print("Confusion Matrix | Skenario 1")
plot_confusion_matrix(confusion_mtx, classes = categories)

"""ROC Curve"""

pred_val = model1.predict(X_val)
fpr, tpr, _ = roc_curve(Y_val, pred_val)
roc_auc = auc(fpr, tpr)

plt.figure()
lw = 2
plt.plot(fpr, tpr, color='#ffba93',
         lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='#7579e7', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Skenario Model 1')
plt.legend(loc="lower right")
plt.show()

"""Nilai AUC Score Skenario Model 1"""

auc_score = roc_auc_score(Y_val, pred_val)
fpr, tpr, th = roc_curve(Y_val, pred_val)
print('AUC Score Skenario 1 = ', round(auc_score, 3))

"""##**[6.2] Skenario Model 2**
**Model Usulan + Hyperparameter Tuning** \
Optimizer : Adam , APL Dropout : 0.05, Dropout : 0.25, Dense : 128
"""

model2 = Sequential(
      [
      Input(shape=(128,128,3)),

      Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),

      Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='swish'),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(0.05),

      Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),

      Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(0.05),

      Conv2D(filters=256, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(0.05),
       
      Conv2D(filters=256, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(0.05),
       
      GlobalMaxPool2D(),
      Flatten(),

      Dense(128, activation = 'swish'),
      Dense(128, activation = 'swish'),
      Dropout(0.25),
      Dense(1, activation='sigmoid')
      ]
  )

"""Skenario model 2 menggunakan Optimizer Adam dengan learning rate 0.00146 dan loss binary crossentropy"""

Adam(learning_rate=0.00146, name='Adam')
model2.compile(optimizer = 'Adam',loss = 'binary_crossentropy',metrics = ['accuracy'])

"""Implementasi Callback berupa Learning rate Scheduler"""

def scheduler(epoch, lr):
  if epoch < 5:
    return lr
  else:
    return lr * tf.math.exp(-0.1)

lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)
tb_callback = tf.keras.callbacks.TensorBoard(
    log_dir='logs', histogram_freq=0, write_graph=True, write_images=False,
    update_freq='epoch', embeddings_freq=0,
    embeddings_metadata=None
)

"""###**[6.2.1]Training Skenario 2**"""

import time

start = time.time()
datagen.fit(X_train)
history2 = model2.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = 100, validation_data = (X_val,Y_val),
                              steps_per_epoch = X_train.shape[0] // batch_size,
                              callbacks=[lr_schedule,tb_callback])
stop = time.time()

print(f'{stop-start}')

model2.save('Model_Skenario2.h5')

"""###**[6.2.2] Visualisasi Skenario 2**"""

acc = history2.history["accuracy"]
val_acc = history2.history["val_accuracy"]
loss = history2.history["loss"]
val_loss = history2.history["val_loss"]

epochs = range(len(acc))

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 5])
ax1.plot(epochs, acc, 'k')
ax1.plot(epochs, val_acc, 'c')
ax1.set_title('Accuracy | Skenario 2')
ax1.legend(['Model accuracy','Model Val accuracy'])

ax2.plot(epochs, loss, 'k')
ax2.plot(epochs, val_loss, 'c')
ax2.set_title('Loss | Skenario 2')
ax2.legend(['Model loss','Model Val loss'])

plt.show()

"""###**[6.2.3] Evaluasi Skenario 2**

Evaluasi pada Data Train
"""

loss, acc = model2.evaluate(X_train,Y_train,verbose = 0)
print("Training Loss {:.5f} dan Training Accuracy {:.2f}%".format(loss,acc*100))

"""Evaluasi pada Data Validation"""

loss, acc = model2.evaluate(X_val,Y_val,verbose = 0)
print("Validation Loss {:.5f} dan Validation Accuracy {:.2f}%".format(loss,acc*100))

"""Evaluasi pada Data Test"""

loss, acc = model2.evaluate(X_test,Y_test,verbose = 0)
print("Test Loss {:.5f} dan Test Accuracy {:.2f}%".format(loss,acc*100))

"""Classification Report"""

print("Skenario Model 2")
pred = model2.predict(X_val)
labels = (pred > 0.5).astype(np.int)

print(classification_report(Y_val, labels, target_names = categories))

"""Confusion Matrix"""

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.BuGn):

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

confusion_mtx = confusion_matrix(Y_val, labels)
print("Skenario Model 2")
plot_confusion_matrix(confusion_mtx, classes = categories)

"""ROC Curve"""

pred_val = model2.predict(X_val)
fpr, tpr, _ = roc_curve(Y_val, pred_val)
roc_auc = auc(fpr, tpr)

plt.figure()
lw = 2
plt.plot(fpr, tpr, color='#ffba93',
         lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='#7579e7', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Skenario Model 2')
plt.legend(loc="lower right")
plt.show()

"""Nilai AUC Score Skenario Model 2"""

auc_score = roc_auc_score(Y_val, pred_val)
fpr, tpr, th = roc_curve(Y_val, pred_val)
print('AUC Score  Skenario Model 2 = ', round(auc_score, 3))

"""##**[6.3] Skenario Model 3**
**Model Usulan + Hyperparameter Tuning + Filter Gabor** \
Optimizer : Adam , APL Dropout : 0.05, Dropout : 0.25, Dense : 128

Impementasi filter gabor pada konvolusi pertama setelah input layer
"""

from keras import backend as K

model3 = Sequential(
      [
      Input(shape=(128,128,1)),

      Conv2D(filters=32, kernel_size=3, padding='same', kernel_initializer=custom_gabor, data_format='channels_last', strides=1, activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),

      Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='swish'),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(0.05),

      Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),

      Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(0.05),

      Conv2D(filters=256, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(0.05),
       
      Conv2D(filters=256, kernel_size=3, strides=1, padding='same', activation='swish'),
      BatchNormalization(),
      MaxPooling2D(pool_size = (2,2)),
      Dropout(0.05),
       
      GlobalMaxPool2D(),
      Flatten(),

      Dense(128, activation = 'swish'),
      Dense(128, activation = 'swish'),
      Dropout(0.25),
      Dense(1, activation='sigmoid')
      ]
  )

"""Skenario model 3 menggunakan Optimizer Adam dengan learning rate 0.00146 dan loss binary crossentropy"""

Adam(learning_rate=0.00146, name='Adam')
model3.compile(optimizer = 'Adam',loss = 'binary_crossentropy',metrics = ['accuracy'])

"""Implementasi Callback berupa Learning rate Scheduler"""

filepath="/content/drive/MyDrive/[Trial] Simponia/Model_Skenario3.h5"

checkpoint = callbacks.ModelCheckpoint(
                                       filepath,
                                       monitor='val_accuracy',
                                       verbose=1,
                                       save_best_only=True,
                                       mode='auto',
                                       )

def scheduler(epoch, lr):
  if epoch < 5:
    return lr
  else:
    return lr * tf.math.exp(-0.1)

lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)

"""###**[6.3.1] Training Skenario 3**"""

import time

start = time.time()
datagen.fit(X_train_gabor)
history3 = model3.fit_generator(datagen.flow(X_train_gabor,Y_train_gabor, batch_size=batch_size),
                              epochs = 100, validation_data = (X_val_gabor,Y_val_gabor),
                              steps_per_epoch = X_train_gabor.shape[0] // batch_size,
                              callbacks=[lr_schedule,checkpoint])
stop = time.time()

print(f'{stop-start}')

model3.save('Model_Skenario3.h5')

# model3 = load_model('Model_Skenario3.h5')

"""###**[6.3.2] Visualisasi Skenario 3**"""

acc = history3.history["accuracy"]
val_acc = history3.history["val_accuracy"]
loss = history3.history["loss"]
val_loss = history3.history["val_loss"]

epochs = range(len(acc))

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 5])
ax1.plot(epochs, acc, 'k')
ax1.plot(epochs, val_acc, 'c')
ax1.set_title('Accuracy | Skenario 3')
ax1.legend(['Model accuracy','Model Val accuracy'])

ax2.plot(epochs, loss, 'k')
ax2.plot(epochs, val_loss, 'c')
ax2.set_title('Loss | Skenario 3')
ax2.legend(['Model loss','Model Val loss'])

plt.show()

"""###**[6.3.3] Evaluasi Skenario 3**

Evaluasi pada Data Train
"""

loss, acc = model3.evaluate(X_train_gabor,Y_train_gabor,verbose = 0)
print("Training Loss {:.5f} dan Training Accuracy {:.2f}%".format(loss,acc*100))

"""Evaluasi pada Data Validation"""

loss, acc = model3.evaluate(X_val_gabor,Y_val_gabor,verbose = 0)
print("Validation Loss {:.5f} dan Validation Accuracy {:.2f}%".format(loss,acc*100))

"""Evaluasi pada Data Test"""

loss, acc = model3.evaluate(X_test_gabor,Y_test_gabor,verbose = 0)
print("Test Loss {:.5f} dan Test Accuracy {:.2f}%".format(loss,acc*100))

"""Classification Report"""

print("Skenario Model 3")
pred = model3.predict(X_val_gabor)
labels = (pred > 0.5).astype(np.int)

print(classification_report(Y_val_gabor, labels, target_names = categories))

"""Confusion Matrix"""

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.GnBu):

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

confusion_mtx = confusion_matrix(Y_val_gabor, labels)
print("Skenario Model 3")
plot_confusion_matrix(confusion_mtx, classes = categories)

"""ROC Curve"""

pred_val = model3.predict(X_val_gabor)
fpr, tpr, _ = roc_curve(Y_val_gabor, pred_val)
roc_auc = auc(fpr, tpr)

plt.figure()
lw = 2
plt.plot(fpr, tpr, color='#ffba93',
         lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='#7579e7', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Skenario Model 3')
plt.legend(loc="lower right")
plt.show()

"""Nilai AUC Score Skenario Model 3"""

auc_score = roc_auc_score(Y_val_gabor, pred_val)
fpr, tpr, th = roc_curve(Y_val_gabor, pred_val)
print('AUC Score Skenario Model 3 = ', round(auc_score, 3))